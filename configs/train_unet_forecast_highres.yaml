seed_everything: 42

# ---------------------------- TRAINER -------------------------------------------
trainer:
  default_root_dir: ${oc.env:AMLT_OUTPUT_DIR,/home/tungnd/climate_joint_training/outputs/forecast_highres/unet}

  precision: 16

  gpus: null
  num_nodes: 1
  accelerator: gpu
  strategy: ddp_find_unused_parameters_false

  min_epochs: 1
  max_epochs: 20
  enable_progress_bar: true

  sync_batchnorm: True
  enable_checkpointing: True
  resume_from_checkpoint: null

  # debugging
  fast_dev_run: false

  logger:
    class_path: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
    init_args:
      save_dir: ${trainer.default_root_dir}/logs
      name: null
      version: null
      log_graph: False
      default_hp_metric: True
      prefix: ""

  callbacks:
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: "step"

    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        dirpath: "${trainer.default_root_dir}/checkpoints"
        monitor: "val/w_rmse" # name of the logged metric which determines when model is improving
        mode: "min" # "max" means higher metric value is better, can be also "min"
        save_top_k: 1 # save k best models (determined by above metric)
        save_last: True # additionaly always save model from last epoch
        verbose: False
        filename: "epoch_{epoch:03d}"
        auto_insert_metric_name: False

    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: "val/w_rmse" # name of the logged metric which determines when model is improving
        mode: "min" # "max" means higher metric value is better, can be also "min"
        patience: 5 # how many validation epochs of not improving until training stops
        min_delta: 0. # minimum change in the monitored metric needed to qualify as an improvement

    - class_path: pytorch_lightning.callbacks.RichModelSummary
      init_args:
        max_depth: -1

    - class_path: pytorch_lightning.callbacks.RichProgressBar

# ---------------------------- MODEL -------------------------------------------
model:
  optimizer: "adam"
  lr: 0.0005
  weight_decay: 1e-5
  warmup_epochs: 5
  max_epochs: 30
  warmup_start_lr: 1e-8
  eta_min: 1e-8

  net:
    class_path: src.models.components.unet.Unet
    init_args:
      in_channels: 24
      hidden_channels: 64
      out_channels: 3
      n_blocks: 2

# ---------------------------- DATA -------------------------------------------
data:
  root_dir: /data0/datasets/weatherbench/data/weatherbench/era5/2.8125deg_forecast_3days
  variables: [
    "t2m",
    "u10",
    "v10",
    "z_50",
    "z_250",
    "z_500",
    "z_600",
    "z_700",
    "z_850",
    "z_925",
    "t_50",
    "t_250",
    "t_500",
    "t_600",
    "t_700",
    "t_850",
    "t_925",
    "q_50",
    "q_250",
    "q_500",
    "q_600",
    "q_700",
    "q_850",
    "q_925",
  ]
  out_variables: ["z_500", "t_850", "t2m"]
  pred_range: 72
  # history: 1
  # window: 0
  subsample: 1
  buffer_size: 1000
  batch_size: 128
  num_workers: 1
  pin_memory: False

